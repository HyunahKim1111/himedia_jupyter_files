01. 데이터 전처리(EDA)

- 변수가 무슨 타입이냐? 독립변수는 수치형이니 명목형이니 종속변수는 수치형이니 명목형이니

컬럼 보기
 - data.columns

전체 컬럼을 보게 하기.
 -  pd.set_option("display.max_columns",None) # 변수가 많을 때 모든 변수를 볼 수 있게 하는 코드.

** 컬럼이 너무 많을 때 몇몇 컬럼만 뽑아서 쓸 경우 이렇게 한다.
data = data[['id', 'title', 'genres', 'vote_average', 'popularity', 'keywords', 'overview']]

** 데이터를 html로 저장해서 EDA를 편하게 볼 수 있는 툴!

from ydata_profiling import ProfileReport
profile = ProfileReport(data, title="Profile Report")
profile.to_file("student_mat.html")


---------------------------------------------------------------------------------------------------------------------------

1. 데이터 타입 맞추기
astype(int, float, str)

data1['serv_ratio_by_age'] = data1['serv_ratio_by_age'].astype(int) 

#만약 금액에 ,가 있어서 int로 바꾸지 못하면 ,를 제외하고 바꿔야 한다.
data['거래금액(만원)'] = data['거래금액(만원)'].str.replace(',', '').astype(int)


1. drop
dropna(), fillna(), isna().sum()
drop('컬럼명', axis=1)
X2 = data2.drop('Survived', axis=1 # axis=1 옆으로 찾아라)
data.drop('N_Sex', axis=1 # axis=1 옆으로 찾아라)

---------------------------------------------------------------------------------------

2. 결측값 확인 및 처리

data.isna().sum() # 결측값이 있는 행을 볼 수 있다.
data.dropna() # 할당은 안 됨. 너무 많은 삭제가 있어서 많이 쓰지는 않는다.

  1) 평균 대치법 : 컬럼에 있는 데이터 값의 평균으로 결측값을 대치한다.

**fillna(대치값)
data['Age'].mean() # 평균값을 찾아서
data['Age'].fillna(data['Age'].mean()) # 평균값으로 결측값을 대치
data['Age'].fillna(24) # 값을 그냥 넣어서 대치할 수도 있어.

  2) Scikit-learn의 SimpleImputer로 평균, 중앙, 최빈값으로 대치하기

from sklearn.impute import SimpleImputer

data[data['Age'].isna() == True] # 결측값이 있는 애들만 가져올 수 있어.
data[data['Age'].isna() == True].index # 결측값이 있는 행의 인덱스를 뽑을 수 있어.
na_indices = data[data['Age'].isna() == True].index

*mean
imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')
imp_mean.fit_transform(data['Age'].values.reshape(-1,1))[:,0]
data.iloc[na_indices] # 결측값으로 묶어놨던 애들에게 평균값으로 부여해주기.

* median
imp_median = SimpleImputer(missing_values=np.nan, strategy='median')
imp_median.fit_transform(data['Age'].values.reshape(-1,1))[:,0]

*most_frequent
imp_most_frequent = SimpleImputer(missing_values=np.nan, strategy='most_frequent')
imp_most_frequent.fit_transform(data['Age'].values.reshape(-1,1))[:,0]

  3) K-최근접이웃(KNN)을 활용해서 결측값 대치

from sklearn.impute import KNNImputer
KNN_imputer = KNNImputer(n_neighbors=1) #n_neighbors=1 기본은 5개야.
data['Age'] = KNN_imputer.fit_transform(data['Age'].values.reshape(-1,1))[:,0]
data.iloc[na_indices]

컬럼 삭제하기
데이터프레임.drop('컬럼명', axis=1)
행 삭제하기
데이터프레임.drop(인덱스)
inplace=True (재할당 없이 바로 반영)

data.drop('Cabin', axis=1, inplace=True) # 재할당을 하거나 inplace=True로 데이터를 저장할 수 있음.

--------------------------------------------------------------------------------------

3. 이상값 탐지 및 처리

data.plot(kind='box') # 박스플랏 위 아래에 찍긴 동그라미들이 이상값이다.
data['Fare'].plot(kind='box') # 행 하나만 박스플랏으로 보기.

# IQR
out_max = x.loc['75%'] + (1.5*(x.loc['75%']-x.loc['25%']))
out_min = x.loc['25%'] - (1.5*(x.loc['75%']-x.loc['25%']))

# 아웃라이어를 계산하는 함수. 우리가 만들어야 해.
def outlier(x):
    x = x.describe()
    out_max = x.loc['75%'] + (1.5*(x.loc['75%']-x.loc['25%']))
    out_min = x.loc['25%'] - (1.5*(x.loc['75%']-x.loc['25%']))
    ol_result = pd.DataFrame([out_max, out_min], columns=out_max.index, index=['상한값','하한값'])
    result=pd.concat([x, ol_result])
    return result

-----------------------------------------------------------------------------------

4.Feature Enginnering, Feature Selection
  1) 파생변수 만들기
  2) 변수선택
data['family'] = data['SibSp'] + data['Parch'] # 새로운 변수 만들때는 데이터프레임.['']
  3) 더미변수 만들기
pd.get_dummies(data, columns=['Sex','Age','Embarked'], drop_first=True) 

-------------------------------------------------------------------------------------

5. 더미변수화(onehot encoding)
범주형 변수들(오류나니까)을 숫자로 변환 (True/False)
카테고리 이름을 맞춰서 컬럼 만들기. True/False로 만들어서 컴퓨터가 숫자로 읽으라고.

data = pd.get_dummies(data, drop_first=True)

-----------------------------------------------------------------------------------

6. 데이터를 훈련데이터/테스트 데이터로 나누기

from sklearn.model_selection import train_test_split
X1 = data1.drop('Survived', axis=1)
y1 = data1['Survived']
X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=10)

7. 정규화(스케일링) : 단위차이를 맞춰줌
독립변수간 단위 차이가 많이 나니까 정규화를 해줄 필요가 있구나. 스케일링이 필요.
* 1) StandardScaler(기본 스케일러, 평균과 표준편차를 사용함.):
평균을 제거하고 데이터를 단위 분산으로 조정함. 그러나 이상치가 있다면 평균과 표준편차에 영향을 미쳐 변환된 데이터의 확산은 매우 달라지게 된다. 
따라서 **이상치가 있는 경우 균형잡힌 척도를 보장할 수 없다는 단점이 있다.**

* 2) MinMaxScaler(최대/최소값이 각각 1,0이 되도록 스케일링): 
모든 Feature(독립변수)값이 0~1사이에 있도록 데이터를 재조정한다.
 다만, 이상치가 있는 경우 변환된 값이 매우 좁은 범위로 압축될 수 있다. 
즉, MinMaxScaler역시 **이상치에 매우 민감**하다.

* 3) MaxAbsScaler(최대 절대값과 0이 각각 1과 0이 되도록 스케일링): 
절대값이 0-1사이에 매핑되도록 한다. -1~1 사이로 재조정한다. 
양수 데이터로만 구성된 특징 데이터셋에서는 MinMax 스케일러와 매우 유사하게 동작하며, **큰 이상지에 민감할 수 있다.**

* 4) RobustScaler(중앙값(median)과 IQR(4분위수) 사용, 아웃라이어의 영향을 최소화): 
아웃라이어의 영향을 최소화한 기법이다. 중앙값(median)과 IQR(사분위수)를 사용하기때문에 StandardScaler와 비교해보면 표준화 후 동일한 값을 더 넓게 분포시키고 있음을 확인할 수 있다.

1,450,000,000,000