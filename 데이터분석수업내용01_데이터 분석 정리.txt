데이터 분석 정리

앙상블 분석

예측력을 높이기 위해서 여러 번의 데이터 분할을 통해 구축된 다수의 모형을 결합해서 새로운 모형을 만드는 방법
앙상블(Ensemble) 분석의 종류
배깅(Bsgging)
부스팅(Boosting)
ADABoost(에이다 부스트)
XGBoost(XG 부스트)
Gradient Boost(그래디언트 부스트)
LightGBM (라이트 GBM)
랜덤배깅(Random Bagging)
Random Forest(랜덤 포레스트)

--------------------------------------------------------------------------------------------------------------------------------

배깅(Bsgging)
여러 개의 부트스트랩을 생성해서 각각을 분류기에 넣고 분석한 결과는 집계하는 알고리즘
여러 개의 분류기에 의한 결과를 놓고 다수결에 의하여 최종 결과값을 선정하는 보팅(voting)으로 결정

from sklearn.ensemble import BaggingClassifier
from sklearn.metrics import accuracy_score, classification_report

bcf = BaggingClassifier(n_estimators=1000, n_jobs=-1, random_state = 10)
bcf.fit(X_train, y_train)
pred_bcf = bcf.predict(X_test)
print(accuracy_score(y_test, pred_bcf))
print(classification_report(y_test, pred_bcf))

---------------------------------------------------------------------------------------------------------------------------------

부스팅
1. 에이다 부스팅 ADABoosting
오류에 가중치를 부여하면서 부스팅을 수행하는 대표적인 알고리즘
요류를 최소화 하는 방향성을 가지고 반복적으로 가중치를 업데이트 해서 성능 향상
DecisionTreeClassifier(max_depth=1)을 사용하는 아주 약한 학습기
순차적으로 학습해야 하므로 멀티코어 사용불가, 분석시간 오래걸림
하이퍼파라미터
n_estimators: int, default=50
learning_rate: float, default=1.0
algorithm: {'SAMME', 'SAMMER.R'}, default='SAMMER.R'


from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(n_estimators = 1000, random_state=10)
ada.fit(X_train, y_train)
pred_ada = ada.predict(X_test)
print(accuracy_score(y_test, pred_ada))
print(classification_report(y_test, pred_ada))

----------------------------------------------------------------------------------------------------------------------------------------------
2. 그래디언트부스팅 GradientTreeBoosting
이전 트리의 오차를 보완하는 방식으로 순차적 트리 생성
무작위성은 없지만 강력한 사전 가지치지가 사용 됨
오류에 가중치를 부여하면서 부스팅을 수행하는 점에서는 동일하나 가중치 업데이트를 경사하강법을 이요해 한다는 점이 다름
약한 학습기의 순차적인 예측 오류 보정을 통해서 학습을 수행하므로 멀티코어 병렬처리가 안됨 => 분석 시간이 오래 걸림
하이퍼파라미터
loss: 경사 하강법에 사용할 비용 함수
learning_rate: 학습율 0~1 사이의 값 작은 값일 수록 예측성능이 좋아질 수 있으나 시간이 오래 걸리고 너무 작으면 지역해에 빠질 수 있다.
n_estimators: 기본 100, 약학습기의 숫자가 많을 수록 예측 성능 향상되나 과적합 우려가 있음.
learning_rate를 작게하고 n_estimators를 크게하면 한계점까지 예측 성능이 좋아질 수 있음
max_depth: defalut= 3 트리의 복잡도가 낮아지도록 5를 넘지 않게 세팅

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(learning_rate=0.01, n_estimators=10000, random_state=10)
gbc.fit(X_train, y_train)
pred_gbc = gbc.predict(X_test)
print(accuracy_score(y_test, pred_gbc))
print(classification_report(y_test, pred_gbc))

-------------------------------------------------------------------------------------------------------------------------------------------------

히스토그램 그래디언트 부스팅(HistGradientBoostingClassifier)
데이터가 10만개 이상일 경우 사용
그래디언트 부스팅 보다 속도가 빠름

from sklearn.ensemble import HistGradientBoostingClassifier
hbc = HistGradientBoostingClassifier(random_state=10)
hbc.fit(X_train, y_train)
pred_hbc = hbc.predict(X_test)
print(accuracy_score(y_test, pred_hbc))
print(classification_report(y_test, pred_hbc))

---------------------------------------------------------------------------------------------------------------------------------------------------

XGBoost 현재 가장 우수한 알고리즘 중 하나
그래디언트 부스팅의 느린 속도 및 과적합 문제를 해결
멀티 코어 분석 지원
자체 과적합 규제 기능
더 이상 긍정적인 영향이 없는 가지에 대한 가지치기 기능
교차검증 기능 자체 내장, 조기 중단 가능
결측값 자체 처리
주요 하이퍼파라미터
learning_rate: 작게
n_estimators: 크게
early stopping관련 옵션
early_stopping_rounds=100
eval_metric='logloss'
eval_set=[(X_test, y_test)]
verbose = True

from xgboost import XGBClassifier
xgb = XGBClassifier(n_estimators= 1000, n_jobs=-1, learning_rate=0.01, random_state=10)
xgb.fit(X_train, y_train)
pred_xgb = xgb.predict(X_test)
print(accuracy_score(y_test, pred_xgb))
print(classification_report(y_test, pred_xgb))

---------------------------------------------------------------------------------------------------------------------------------------------------------

LightGBM
XGBoost에 비해서 속도가 빠르고 메모리 사용량이 적음
10,000건 이하의 크기가 작은 데이터 세트에서 과적합 발생 위험이 있음
보통 트리기반 알고리즘은 균형 트리 분할을 사용해 트리간 균형을 맞추기 때문에 시간이 오래걸림
라이트GBM은 Leaf중심 트리 분할을 사용하여 트리의 균형을 맞추지 않고 최대 손실값을 가지는 리프노드를 지속적으로 분할하면서 깊고 비대칭적인 트리생성

from lightgbm import LGBMClassifier
lgbm = LGBMClassifier(n_estimators= 1000, n_jobs=-1, learning_rate=0.01, random_state=10)
lgbm.fit(X_train, y_train)
pred_lgbm = lgbm.predict(X_test)
print(accuracy_score(y_test, pred_lgbm))
print(classification_report(y_test, pred_lgbm))

------------------------------------------------------------------------------------------------------------------------------------------------------

RandomForest

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators= 1000, n_jobs=-1, random_state=10)
rfc.fit(X_train, y_train)
pred_rfc = rfc.predict(X_test)
print(accuracy_score(y_test, pred_rfc))
print(classification_report(y_test, pred_rfc))

-------------------------------------------------------------------------------------------------------------------------------------------------------

ExtraTreesClassifier
랜덤포레스트와 비슷하지만 후보 특성을 무작위로 분할 한 다음 최적의 분할 탐색
부트스트랩 샘플링은 사용하지 않음
무작위성 증가 => 모델의 편향은 늘어나지만 분산이 감소

from sklearn.ensemble import ExtraTreesClassifier
etc = ExtraTreesClassifier(n_estimators= 1000, n_jobs=-1, random_state=10)
etc.fit(X_train, y_train)
pred_etc = etc.predict(X_test)
print(accuracy_score(y_test, pred_etc))
print(classification_report(y_test, pred_etc))

-----------------------------------------------------------------------------------------------------------------------------------------------------

로지스틱 회귀분석 

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

lr = LogisticRegression()
lr.fit(X_train, y_train)
pred = lr.predict(X_test)
print(accuracy_score(y_test, pred))
print(confusion_matrix(y_test, pred))
print(classification_report(y_test, pred))

---------------------------------------------------------------------------------------------------------------------------------------------------

